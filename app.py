import requests
from bs4 import BeautifulSoup
import tldextract
import whois
from transformers import pipeline
import streamlit as st
from datetime import datetime

# -----------------------
# Load AI model
# -----------------------
@st.cache_resource
def load_model():
    return pipeline("zero-shot-classification",
                    model="joeddav/xlm-roberta-large-xnli")

classifier = load_model()

# -----------------------
# Crawl n·ªôi dung website
# -----------------------
def crawl_page(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        text = " ".join([p.get_text() for p in soup.find_all("p")])
        return text[:2000]
    except Exception as e:
        return f"L·ªói khi crawl: {e}"

# -----------------------
# Ph√¢n t√≠ch domain
# -----------------------
def analyze_domain(url):
    ext = tldextract.extract(url)
    domain = f"{ext.domain}.{ext.suffix}"
    suspicious_score = 0

    if ext.suffix not in ["org", "com", "net", "vn"]:
        suspicious_score += 20
    if any(word in ext.domain.lower() for word in ["jesus", "bible", "vatican", "church"]):
        suspicious_score += 10
    try:
        domain_info = whois.whois(domain)
        if domain_info.creation_date:
            creation_date = domain_info.creation_date[0] if isinstance(domain_info.creation_date, list) else domain_info.creation_date
            if (datetime.now() - creation_date).days < 365:
                suspicious_score += 30
    except:
        suspicious_score += 20

    return domain, suspicious_score

# -----------------------
# Ph√¢n t√≠ch n·ªôi dung b·∫±ng AI
# -----------------------
def analyze_content(text):
    if "L·ªói khi crawl" in text:
        return 0, "unknown"

    labels = ["legit", "fake", "scam"]
    result = classifier(text, labels)
    label = result["labels"][0]
    score = result["scores"][0]

    suspicious_score = 0
    if label == "fake":
        suspicious_score += 40
    elif label == "scam":
        suspicious_score += 70

    return suspicious_score, f"{label} ({round(score*100,2)}%)"

# -----------------------
# Dashboard Streamlit
# -----------------------
st.set_page_config(page_title="Fake Website Detector", layout="centered")
st.title("üõ°Ô∏è Fake Website Detector (Kinh Th√°nh & Kit√¥ gi√°o)")

url = st.text_input("üîó Nh·∫≠p URL website c·∫ßn ki·ªÉm tra:")

if st.button("Ki·ªÉm tra"):
    if url:
        st.write(f"ƒêang ki·ªÉm tra: **{url}**")

        # Crawl + ph√¢n t√≠ch
        text = crawl_page(url)
        domain, score_domain = analyze_domain(url)
        score_content, label_ai = analyze_content(text)

        # T·ªïng ƒëi·ªÉm
        total_score = score_domain + score_content
        risk_level = "‚úÖ An to√†n"
        color = "green"
        if total_score >= 70:
            risk_level = "üö® Nguy hi·ªÉm"
            color = "red"
        elif total_score >= 40:
            risk_level = "‚ö†Ô∏è ƒê√°ng ng·ªù"
            color = "orange"

        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        st.subheader("K·∫øt qu·∫£ ph√¢n t√≠ch")
        st.markdown(f"- **Domain**: {domain}")
        st.markdown(f"- **ƒêi·ªÉm domain**: {score_domain}")
        st.markdown(f"- **AI ph√¢n lo·∫°i**: {label_ai}")
        st.markdown(f"- **ƒêi·ªÉm n·ªôi dung (AI)**: {score_content}")
        st.markdown(f"- **‚û°Ô∏è T·ªïng ƒëi·ªÉm r·ªßi ro**: {total_score} | <span style='color:{color}'>{risk_level}</span>", unsafe_allow_html=True)

        # Xem tr∆∞·ªõc n·ªôi dung crawl
        with st.expander("üìÑ Xem n·ªôi dung crawl ƒë∆∞·ª£c"):
            st.write(text)
    else:
        st.warning("Vui l√≤ng nh·∫≠p URL tr∆∞·ªõc khi ki·ªÉm tra.")
