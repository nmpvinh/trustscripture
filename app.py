import streamlit as st
import requests
import tldextract
import whois
from bs4 import BeautifulSoup
from transformers import pipeline

# ========== 1. Load model Hugging Face ==========
@st.cache_resource
def load_model():
    return pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")

nlp_model = load_model()

# ========== 2. Rule-based checks ==========
def rule_based_check(url: str):
    issues = []
    score = 0

    # Extract domain
    domain_info = tldextract.extract(url)
    domain = f"{domain_info.domain}.{domain_info.suffix}"

    # Rule 1: URL quÃ¡ dÃ i
    if len(url) > 75:
        issues.append("URL quÃ¡ dÃ i, cÃ³ thá»ƒ lÃ  dáº¥u hiá»‡u giáº£ máº¡o.")
        score += 1

    # Rule 2: WHOIS áº©n
    try:
        w = whois.whois(domain)
        if not w.organization:
            issues.append("WHOIS bá»‹ áº©n hoáº·c thiáº¿u thÃ´ng tin.")
            score += 1
    except Exception:
        issues.append("KhÃ´ng truy xuáº¥t Ä‘Æ°á»£c WHOIS.")
        score += 1

    # Rule 3: Domain miá»…n phÃ­ hoáº·c báº¥t thÆ°á»ng
    if domain_info.suffix in ["tk", "ml", "ga", "cf", "gq"]:
        issues.append(f"Domain sá»­ dá»¥ng TLD miá»…n phÃ­: {domain_info.suffix}")
        score += 1

    return score, issues

# ========== 3. AI-based checks ==========
def ai_based_check(url: str):
    try:
        response = requests.get(url, timeout=5)
        soup = BeautifulSoup(response.text, "html.parser")
        text_content = " ".join(soup.stripped_strings)[:500]  # láº¥y 500 kÃ½ tá»± Ä‘áº§u
        result = nlp_model(text_content)[0]
        return result
    except Exception as e:
        return {"label": "ERROR", "score": 0.0, "error": str(e)}

# ========== 4. Streamlit UI ==========
st.set_page_config(page_title="Fake Website Detector", page_icon="ğŸ›¡ï¸", layout="wide")

st.title("ğŸ›¡ï¸ Fake Website Detector")
st.markdown("CÃ´ng cá»¥ phÃ¡t hiá»‡n website giáº£ máº¡o / lá»«a Ä‘áº£o vá» KitÃ´ giÃ¡o & Kinh ThÃ¡nh.")

url = st.text_input("ğŸ”— Nháº­p URL Ä‘á»ƒ kiá»ƒm tra:", "https://example.com")

if st.button("Kiá»ƒm tra"):
    if not url.startswith("http"):
        st.warning("âš ï¸ HÃ£y nháº­p URL Ä‘áº§y Ä‘á»§ (bao gá»“m http:// hoáº·c https://)")
    else:
        with st.spinner("Äang phÃ¢n tÃ­ch..."):
            # Rule-based
            rule_score, rule_issues = rule_based_check(url)

            # AI-based
            ai_result = ai_based_check(url)

        # ========== Hiá»ƒn thá»‹ káº¿t quáº£ ==========
        st.subheader("ğŸ“Š Káº¿t quáº£ kiá»ƒm tra")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("### Rule-based check")
            if rule_issues:
                for issue in rule_issues:
                    st.error(f"- {issue}")
            else:
                st.success("KhÃ´ng phÃ¡t hiá»‡n dáº¥u hiá»‡u Ä‘Ã¡ng ngá».")

        with col2:
            st.markdown("### AI-based check")
            if ai_result.get("label") == "ERROR":
                st.error(f"Lá»—i khi phÃ¢n tÃ­ch ná»™i dung: {ai_result.get('error')}")
            else:
                label = ai_result["label"]
                score = round(ai_result["score"] * 100, 2)
                if label == "NEGATIVE":
                    st.error(f"AI Ä‘Ã¡nh giÃ¡ ná»™i dung KHáº¢ NGHI ({score}%)")
                else:
                    st.success(f"AI Ä‘Ã¡nh giÃ¡ ná»™i dung AN TOÃ€N ({score}%)")

        # ========== Tá»•ng káº¿t ==========
        risk_level = rule_score
        if ai_result.get("label") == "NEGATIVE":
            risk_level += 1

        st.subheader("ğŸ§¾ Káº¿t luáº­n")
        if risk_level >= 2:
            st.error("âš ï¸ Website cÃ³ kháº£ nÄƒng GIáº¢ Máº O hoáº·c Lá»ªA Äáº¢O.")
        else:
            st.success("âœ… Website cÃ³ váº» an toÃ n.")
